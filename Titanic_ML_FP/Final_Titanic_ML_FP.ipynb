{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic ML from Disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.getcwd()\n",
    "train_data = pd.read_csv(os.path.join(dir,'input', 'train.csv'))\n",
    "test_data = pd.read_csv(os.path.join(dir,'input', 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_eda_info (df, name='DataFrame'):\n",
    "\n",
    "    print(f'=== {name}: Basic Information ===')\n",
    "    print(f'Shape: {df.shape}')\n",
    "\n",
    "    print('='*60)\n",
    "    print('=== Head of DataFrame ===')\n",
    "    display(df.head())\n",
    "\n",
    "    with pd.option_context('display.float_format', '{:,.2f}'.format):\n",
    "        print('=== Numeric Describe ===')\n",
    "        display(df.describe())\n",
    "    \n",
    "    print('='*60)\n",
    "    print('=== Missing Values ===')\n",
    "    print(df.isna().sum())\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pointbiserialr, chi2_contingency\n",
    "\n",
    "def correlation_analysis_type(df, task=\"classification\", target_col=None, numeric_cols=None):\n",
    "    \"\"\"\n",
    "    Function to analyze correlations based on the task (classification or regression)\n",
    "    and plot a heatmap of correlations.\n",
    "    \"\"\"\n",
    "    if target_col is None:\n",
    "        raise ValueError(\"Please specify the target_col (dependent variable).\")\n",
    "\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "    # Exclude the target column from numeric_cols\n",
    "    if target_col in numeric_cols:\n",
    "        numeric_cols.remove(target_col)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        if len(df[target_col].unique()) > 2:\n",
    "            raise ValueError(\"For classification, the target_col must be binary (e.g., 0 or 1).\")\n",
    "        correlations = {}\n",
    "        for col in numeric_cols:\n",
    "            corr, _ = pointbiserialr(df[target_col], df[col])\n",
    "            correlations[col] = corr\n",
    "        # Convert to DataFrame for visualization\n",
    "        corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation'])\n",
    "        corr_df = corr_df.sort_values(by='Correlation', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(corr_df, annot=True, cmap='coolwarm', cbar=False, fmt='.2f')\n",
    "        plt.title('Point-Biserial Correlation Heatmap (Classification)')\n",
    "        plt.show()\n",
    "\n",
    "    elif task == \"regression\":\n",
    "        corr_df = df[numeric_cols + [target_col]].corr()\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "        plt.title('Correlation Heatmap (Regression)')\n",
    "        plt.show()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task. Please choose 'classification' or 'regression'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DataFrame: Basic Information ===\n",
      "Shape: (891, 12)\n",
      "============================================================\n",
      "=== Head of DataFrame ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Numeric Describe ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.00</td>\n",
       "      <td>891.00</td>\n",
       "      <td>891.00</td>\n",
       "      <td>714.00</td>\n",
       "      <td>891.00</td>\n",
       "      <td>891.00</td>\n",
       "      <td>891.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2.31</td>\n",
       "      <td>29.70</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.38</td>\n",
       "      <td>32.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.35</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.84</td>\n",
       "      <td>14.53</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.81</td>\n",
       "      <td>49.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>20.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>38.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>512.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId  Survived  Pclass    Age  SibSp  Parch   Fare\n",
       "count       891.00    891.00  891.00 714.00 891.00 891.00 891.00\n",
       "mean        446.00      0.38    2.31  29.70   0.52   0.38  32.20\n",
       "std         257.35      0.49    0.84  14.53   1.10   0.81  49.69\n",
       "min           1.00      0.00    1.00   0.42   0.00   0.00   0.00\n",
       "25%         223.50      0.00    2.00  20.12   0.00   0.00   7.91\n",
       "50%         446.00      0.00    3.00  28.00   0.00   0.00  14.45\n",
       "75%         668.50      1.00    3.00  38.00   1.00   0.00  31.00\n",
       "max         891.00      1.00    3.00  80.00   8.00   6.00 512.33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "=== Missing Values ===\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "basic_eda_info(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing in train_data:\n",
      "PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "..................................................\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing in train_data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "print(\".\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Missing data on Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_age (df):\n",
    "    df = df.reset_index()\n",
    "    df = df.set_index('PassengerId')\n",
    "\n",
    "    # Extract titles\n",
    "    df['Title'] = df.Name.str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "    # Standarize titles\n",
    "    mapping = {'Mlle':'Miss', 'Ms':'Miss', 'Mme':'Mrs'}\n",
    "    df['Title'] = df['Title'].replace(mapping)\n",
    "    title_medians = df.groupby('Title')['Age'].median().to_dict()\n",
    "\n",
    "    # Replace missing values in the 'Age' column with the median of the corresponding 'Title'\n",
    "    df['Age'] = df.apply(\n",
    "        lambda row: title_medians[row['Title']] if pd.isnull(row['Age']) else row['Age'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    title_std = ~df['Title'].isin(['Mr', 'Miss', 'Mrs', 'Master'])\n",
    "    df.loc[title_std, 'Title'] = df.loc[title_std, 'Sex'].map({'male': 'Mr', 'female': 'Mrs'})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Missing data on Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_cabin (df):\n",
    "\n",
    "    # Special case for cabins as nan may be signal\n",
    "    df.Cabin = df.Cabin.fillna('Z00 ')\n",
    "\n",
    "    # Create the 'Cabin_2' column\n",
    "    df['Deck'] = df['Cabin'].str.split(' ').str[0].str[0]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Missing data on Embarked & Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_embarked_fare (df):\n",
    "    \n",
    "    df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
    "    df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Featuring Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Merging supplementary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features_columns (df):\n",
    "\n",
    "    # Creating new family_size column\n",
    "    df['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df['IsAlone'] = (df['Family_Size'] == 1).astype(int)\n",
    "\n",
    "    # Creating Fare per person\n",
    "    df['Fare_Per_Person']= df['Fare'] / (df['Family_Size'])\n",
    "        \n",
    "    # Creating Age times class\n",
    "    df['Age*Class'] = df['Age'] * df['Pclass']\n",
    "    df['Age*Fare'] = df['Age'] * df['Fare']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for backup, add_factorize_or_encode do both together\n",
    "\n",
    "def add_dummies (df):\n",
    "    \n",
    "    df_sex = pd.get_dummies(df['Sex'], prefix='sex', drop_first=False, dtype=int)\n",
    "    df_Pclass = pd.get_dummies(df['Pclass'], prefix='class', drop_first=False, dtype=int)\n",
    "    df_Embarked = pd.get_dummies(df['Embarked'], prefix='Embarked', drop_first=False, dtype=int)\n",
    "    df_Title = pd.get_dummies(df['Title'], prefix='Title', drop_first=False, dtype=int)\n",
    "    df_Deck = pd.get_dummies(df['Deck'], prefix='Deck', drop_first=False, dtype=int)\n",
    "\n",
    "    df = pd.concat([df, df_sex, df_Pclass, df_Embarked, df_Title, df_Deck], axis=1)\n",
    "\n",
    "    df = df.drop(['Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Embarked', 'Deck', 'Title', 'Cabin'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for backup, add_factorize_or_encode do both together\n",
    "\n",
    "def add_encoding (df):\n",
    "    \n",
    "    df['Sex'], mapping_index = pd.factorize(df['Sex'])\n",
    "    df['Pclass'], mapping_index = pd.factorize(df['Pclass'])\n",
    "    df['Embarked'], mapping_index = pd.factorize(df['Embarked'])\n",
    "    df['Title'], mapping_index = pd.factorize(df['Title'])\n",
    "    df['Deck'], mapping_index = pd.factorize(df['Deck'])\n",
    "\n",
    "    df = df.drop(['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "def add_factorize_or_encode(df, task = \"classification\"):\n",
    "    # Preprocessing Step: Automatically handle categorical features\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    if task == \"classification\":\n",
    "        # Factorize categorical columns for tree-based models and naive Bayes\n",
    "        if len(categorical_cols) > 0:\n",
    "            print(f\"Factorizing categorical columns: {list(categorical_cols)}\")\n",
    "            for col in categorical_cols:\n",
    "                df[col] = pd.factorize(df[col])[0]  # Factorize categories into numeric codes\n",
    "\n",
    "    elif task == \"regression\":\n",
    "        # One-hot encode categorical columns for regression models\n",
    "        if len(categorical_cols) > 0:\n",
    "            print(f\"One-hot encoding categorical columns: {list(categorical_cols)}\")\n",
    "            encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "            encoded_cols = pd.DataFrame(encoder.fit_transform(X[categorical_cols]),\n",
    "                                        columns=encoder.get_feature_names_out(categorical_cols))\n",
    "            df = pd.concat([df.drop(columns=categorical_cols), encoded_cols], axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task. Please choose 'classification' or 'regression'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Standarizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretise_numeric(train, test, numeric_cols, no_bins=10):\n",
    "    \"\"\"\n",
    "    Discretizes the specified numeric columns into bins, leaving other columns unchanged.\n",
    "    \"\"\"\n",
    "    # Reset indices for consistent processing\n",
    "    train = train.reset_index()\n",
    "    test = test.reset_index()\n",
    "    \n",
    "    # Get the sizes of train and test datasets\n",
    "    N = len(train)\n",
    "    M = len(test)\n",
    "    \n",
    "    # Ensure unique indices for test by offsetting\n",
    "    test.index += N  # Shift test index to avoid overlaps with train\n",
    "    \n",
    "    # Combine train and test for consistent discretization\n",
    "    joint_df = pd.concat([train, test], axis=0)\n",
    "    \n",
    "    # Loop through only the specified numeric_cols to discretize\n",
    "    for column in numeric_cols:\n",
    "        if column in joint_df.columns:\n",
    "            # Apply pd.qcut to discretize into quantile-based bins\n",
    "            joint_df[column] = pd.qcut(joint_df[column], no_bins, labels=False, duplicates='drop')\n",
    "        else:\n",
    "            print(f\"Warning: Column '{column}' not found in the dataset.\")\n",
    "\n",
    "    # Split the combined dataset back into train and test\n",
    "    train = joint_df.iloc[:N].reset_index(drop=True)\n",
    "    test = joint_df.iloc[N:].reset_index(drop=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standarize_values(train, test, numeric_cols):\n",
    "    \"\"\"\n",
    "    Standardizes specified numeric columns in the training and test datasets,\n",
    "    leaving all other columns unchanged.\n",
    "    \"\"\"\n",
    "    # Reset indices for consistent processing\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training set for the specified numeric columns\n",
    "    scaler.fit(train[numeric_cols])\n",
    "\n",
    "    # Transform only the specified numeric columns\n",
    "    train_scaled = scaler.transform(train[numeric_cols])\n",
    "    test_scaled = scaler.transform(test[numeric_cols])\n",
    "\n",
    "    # Create DataFrames for the standardized numeric columns\n",
    "    train_scaled_df = pd.DataFrame(train_scaled, columns=numeric_cols, index=train.index)\n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=numeric_cols, index=test.index)\n",
    "\n",
    "    # Replace the specified numeric columns with the standardized versions\n",
    "    train[numeric_cols] = train_scaled_df\n",
    "    test[numeric_cols] = test_scaled_df\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equal_dataframes (train, test):\n",
    "    \n",
    "    # Get the columns that are in the test DataFrame\n",
    "    common_columns = test.columns\n",
    "\n",
    "    # Add 'Survived' to the list of columns\n",
    "    columns_to_keep = list(common_columns)\n",
    "\n",
    "    # Filter the train DataFrame to keep only these columns\n",
    "    train_filtered = train[columns_to_keep]\n",
    "\n",
    "    # Set PassengerId as Index\n",
    "    # train_filtered = train_filtered.set_index('PassengerId')\n",
    "    # test = test.set_index('PassengerId')\n",
    "\n",
    "    return train_filtered, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Final features & cleaning selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df (df):\n",
    "    \n",
    "    # Do all the steps together\n",
    "    df = replace_missing_age(df)\n",
    "    df = replace_missing_embarked_fare(df)\n",
    "    df = replace_cabin(df)\n",
    "    df = add_features_columns(df)\n",
    "    df = add_factorize_or_encode(df, \"classification\")\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(['Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data set analysis & premodeling work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorizing categorical columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Title', 'Deck']\n",
      "Factorizing categorical columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Title', 'Deck']\n"
     ]
    }
   ],
   "source": [
    "train = clean_df(train_data)\n",
    "test = clean_df(test_data)\n",
    "\n",
    "# Only discretise numerical columns, not binaries\n",
    "tr, te = discretise_numeric(train, test, numeric_cols=['Age', 'Fare'])\n",
    "\n",
    "# Only standarize numerical columns, not discrets, not binaries\n",
    "tr, te = standarize_values(tr, te, numeric_cols=['Fare_Per_Person', 'Age*Class', 'Age*Fare'])\n",
    "train_final, test_final = equal_dataframes(tr, te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_analysis_type(tr, task=\"classification\", target_col=\"Survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_analysis_type(tr, task=\"classification\", target_col=\"Survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Short modeling option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Model, Predicts and creates the submission file\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "y = train_final['Survived']\n",
    "\n",
    "X = train_final.drop(['Survived', 'PassengerId'], axis=1)\n",
    "X_test = test_final.drop(['Survived', 'PassengerId'], axis=1)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1)\n",
    "model.fit(X,y)\n",
    "\n",
    "predictions = model.predict(X_test).astype('int')\n",
    "\n",
    "output = pd.DataFrame({'PassengerID': test_final.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Modeling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "\n",
    "def evaluate_models(X, y, task=\"classification\"):\n",
    "    \"\"\"\n",
    "    Evaluates multiple machine learning models for a given task, and calculates relevant metrics.\n",
    "\n",
    "    Parameters:\n",
    "        X (DataFrame): Features for training.\n",
    "        y (Series): Target variable for training.\n",
    "        task (str): \"classification\" or \"regression\".\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary of models and their evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Dictionary to store results: {model_name: metrics}\n",
    "    results = {}\n",
    "\n",
    "    # List of models to evaluate based on the task\n",
    "    if task == \"classification\":\n",
    "        models = {\n",
    "            'Random Forest Classifier': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1),\n",
    "            'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=1),\n",
    "            'Logistic Regression': LogisticRegression(max_iter=2000, random_state=1),\n",
    "            'Support Vector Machine (SVC)': SVC(kernel='rbf', probability=True, random_state=1),\n",
    "            'K-Nearest Neighbors (KNN) Classifier': KNeighborsClassifier(n_neighbors=5),\n",
    "            'Naive Bayes': GaussianNB(),\n",
    "            'Neural Network Classifier': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=1)\n",
    "        }\n",
    "    elif task == \"regression\":\n",
    "        models = {\n",
    "            'Random Forest Regressor': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=1),\n",
    "            'Gradient Boosting Regressor': GradientBoostingClassifier(random_state=1),\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge Regression': Ridge(random_state=1),\n",
    "            'Support Vector Machine (SVR)': SVR(kernel='rbf'),\n",
    "            'K-Nearest Neighbors (KNN) Regressor': KNeighborsRegressor(n_neighbors=5),\n",
    "            'Neural Network Regressor': MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=1)\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task. Please choose 'classification' or 'regression'.\")\n",
    "\n",
    "    # Loop through each model\n",
    "    for model_name, model in models.items():\n",
    "        # Split the data into training and validation sets (80% train, 20% test)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "        # Fit the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the validation set\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        # Initialize a metrics dictionary\n",
    "        metrics = {}\n",
    "\n",
    "        # Calculate metrics for classification\n",
    "        if task == \"classification\":\n",
    "            metrics['Accuracy'] = accuracy_score(y_val, y_val_pred)\n",
    "            metrics['F1 Score'] = f1_score(y_val, y_val_pred, average='weighted')\n",
    "            if hasattr(model, \"predict_proba\"):  # Check if model supports predict_proba\n",
    "                y_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "                metrics['ROC-AUC'] = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "        # Calculate metrics for regression\n",
    "        elif task == \"regression\":\n",
    "            metrics['MSE'] = mean_squared_error(y_val, y_val_pred)\n",
    "            metrics['MAE'] = mean_absolute_error(y_val, y_val_pred)\n",
    "            metrics['R²'] = r2_score(y_val, y_val_pred)\n",
    "            metrics['RMSE'] = np.sqrt(metrics['MSE'])\n",
    "\n",
    "        # Save the model's metrics in the results dictionary\n",
    "        results[model_name] = metrics\n",
    "\n",
    "    # Transpose to have models as rows and metrics as columns\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    \n",
    "    # Reset the index to add the model names as a column\n",
    "    results_df = results_df.reset_index().rename(columns={'index': 'Model'})\n",
    "        \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_grid(model_name):\n",
    "    \"\"\"\n",
    "    Returns a parameter grid for the selected model.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str): Name of the model for which to generate the param_grid.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with hyperparameter options for the model.\n",
    "    \"\"\"\n",
    "    param_grids = {\n",
    "        'Gradient Boosting Classifier': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        },\n",
    "        'Random Forest Classifier': {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [5, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        'Logistic Regression': {\n",
    "            'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'solver': ['lbfgs', 'liblinear', 'saga']\n",
    "        },\n",
    "        'Support Vector Machine (SVC)': {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        },\n",
    "        'K-Nearest Neighbors (KNN) Classifier': {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "        },\n",
    "        'Naive Bayes': {\n",
    "            # Naive Bayes typically has limited hyperparameters to tune\n",
    "            'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "        },\n",
    "        'Neural Network Classifier': {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'activation': ['relu', 'tanh', 'logistic'],\n",
    "            'solver': ['sgd', 'adam'],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "            'learning_rate': ['constant', 'adaptive']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Return the parameter grid for the selected model\n",
    "    if model_name in param_grids:\n",
    "        return param_grids[model_name]\n",
    "    else:\n",
    "        raise ValueError(f\"Parameter grid for '{model_name}' is not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def search_best_params(X, y, model, param_grid):\n",
    "    \"\"\"\n",
    "    Searches for the best hyperparameters for a given model using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "        X (DataFrame): Features for training.\n",
    "        y (Series): Target variable for training.\n",
    "        model: The machine learning model to optimize.\n",
    "        param_grid (dict): Dictionary containing hyperparameter grid to search.\n",
    "\n",
    "    Returns:\n",
    "        dict: The best parameters and the corresponding score.\n",
    "    \"\"\"\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',  # You can change scoring as per your task\n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        verbose=1,  # Displays the progress\n",
    "        n_jobs=-1  # Uses all available processors\n",
    "    )\n",
    "\n",
    "    # Fit GridSearchCV on the data\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Return the best parameters and score\n",
    "    return {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_score': grid_search.best_score_\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Data set modeling work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.792611</td>\n",
       "      <td>0.848669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.793296</td>\n",
       "      <td>0.787348</td>\n",
       "      <td>0.855260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.766373</td>\n",
       "      <td>0.821530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Support Vector Machine (SVC)</td>\n",
       "      <td>0.592179</td>\n",
       "      <td>0.440498</td>\n",
       "      <td>0.736366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K-Nearest Neighbors (KNN) Classifier</td>\n",
       "      <td>0.564246</td>\n",
       "      <td>0.524568</td>\n",
       "      <td>0.625678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.715084</td>\n",
       "      <td>0.711527</td>\n",
       "      <td>0.808478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neural Network Classifier</td>\n",
       "      <td>0.754190</td>\n",
       "      <td>0.745276</td>\n",
       "      <td>0.827475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Model  Accuracy  F1 Score   ROC-AUC\n",
       "0              Random Forest Classifier  0.798883  0.792611  0.848669\n",
       "1          Gradient Boosting Classifier  0.793296  0.787348  0.855260\n",
       "2                   Logistic Regression  0.770950  0.766373  0.821530\n",
       "3          Support Vector Machine (SVC)  0.592179  0.440498  0.736366\n",
       "4  K-Nearest Neighbors (KNN) Classifier  0.564246  0.524568  0.625678\n",
       "5                           Naive Bayes  0.715084  0.711527  0.808478\n",
       "6             Neural Network Classifier  0.754190  0.745276  0.827475"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_final['Survived']\n",
    "\n",
    "X = train_final.drop(['Survived', 'PassengerId'], axis=1) #Check to leave or not 'index' column, sometimes it predicts better with it\n",
    "X_test = test_final.drop(['Survived', 'PassengerId'], axis=1)\n",
    "\n",
    "results = evaluate_models(X, y, task=\"classification\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Gradient Boosting Classifier?\n",
    "# Highest Accuracy:\n",
    "# It achieved an Accuracy of 0.815642, the highest among all the models. Accuracy measures the proportion of correctly classified samples.\n",
    "\n",
    "# Strong F1 Score:\n",
    "# Its F1 Score is 0.811958, also the highest. F1 Score is particularly important for imbalanced datasets as it balances precisio\n",
    "# (minimizing false positives) and recall (minimizing false negatives).\n",
    "\n",
    "# Best ROC-AUC:\n",
    "# It also has the highest ROC-AUC (0.848798), which evaluates the model's ability to distinguish between classes.\n",
    "# Higher values indicate better overall performance for classification problems, especially when probabilities are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best Parameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Best Cross-Validation Score: 0.8417487916640513\n"
     ]
    }
   ],
   "source": [
    "chosen_model_name = 'Gradient Boosting Classifier'\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "param_grid = get_param_grid(chosen_model_name)\n",
    "\n",
    "# Call the function to find the best parameters\n",
    "best_params_result = search_best_params(X, y, model, param_grid)\n",
    "best_params = best_params_result['best_params']\n",
    "\n",
    "# Display the results\n",
    "print(\"Best Parameters:\", best_params_result['best_params'])\n",
    "print(\"Best Cross-Validation Score:\", best_params_result['best_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Prediction and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Get Predictions & Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the best parameters\n",
    "best_model = GradientBoostingClassifier(\n",
    "    n_estimators=best_params['n_estimators'],\n",
    "    max_depth=best_params['max_depth'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    subsample=best_params['subsample'],\n",
    "    random_state=1  # Keep the random_state consistent\n",
    ")\n",
    "\n",
    "# Fit the model on the full training dataset\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = best_model.predict(X_test).astype('int')\n",
    "\n",
    "# Export to .csv file\n",
    "output = pd.DataFrame({'PassengerID': test_final.PassengerId, 'Survived': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
